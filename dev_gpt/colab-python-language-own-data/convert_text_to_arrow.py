"""
Convert text files back to Arrow dataset format.
This reads the plain text files and creates an Arrow dataset.
"""

import csv
import os
import re

from datasets import Dataset

# Configuration
text_dir = "dataset_text_files"
output_dir = "local_dataset_own"
text_file = os.path.join(text_dir, "all_code.txt")
metadata_file = os.path.join(text_dir, "metadata.csv")

print("Reading text file and metadata...")

# Read metadata first
metadata_dict = {}
with open(metadata_file, "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
        idx = int(row["index"])
        metadata_dict[idx] = row

print(f"Loaded metadata for {len(metadata_dict)} samples")

# Parse the text file to extract code samples
print("Parsing text file...")
samples = []
current_code = []
current_idx = None
current_metadata = {}

with open(text_file, "r", encoding="utf-8") as f:
    lines = f.readlines()
    i = 0

    while i < len(lines):
        line = lines[i]

        # Check for file separator
        if line.strip() == "=" * 80:
            # Save previous sample if exists
            if current_idx is not None and current_code:
                content = "".join(current_code).strip()
                samples.append(
                    {
                        "repo_name": current_metadata["repo_name"],
                        "path": current_metadata["path"],
                        "copies": (
                            int(metadata_dict[current_idx].get("copies", 0))
                            if current_idx in metadata_dict
                            else 0
                        ),
                        "size": int(current_metadata["size"]),
                        "content": content,
                        "license": current_metadata["license"],
                        "hash": 0,  # Placeholder
                        "line_mean": float(current_metadata.get("line_mean", 0)),
                        "line_max": int(current_metadata.get("line_max", 0)),
                        "alpha_frac": 0.0,  # Placeholder
                        "autogenerated": False,
                    }
                )

                if len(samples) % 1000 == 0:
                    print(f"  Parsed {len(samples)} samples...")

            # Start new sample
            current_code = []
            i += 1

            # Parse header line: FILE 1/10000: repo/path
            if i < len(lines):
                header = lines[i].strip()
                match = re.match(r"FILE (\d+)/\d+: (.+)", header)
                if match:
                    current_idx = int(match.group(1)) - 1
                    repo_path = match.group(2)

                    # Split repo_name and path
                    parts = repo_path.split("/", 1)
                    if len(parts) == 2:
                        current_metadata["repo_name"] = parts[0]
                        current_metadata["path"] = parts[1]
                    else:
                        current_metadata["repo_name"] = repo_path
                        current_metadata["path"] = ""
                i += 1

            # Parse metadata line: License: xxx | Size: xxx bytes
            if i < len(lines):
                meta_line = lines[i].strip()
                license_match = re.search(r"License: (\S+)", meta_line)
                size_match = re.search(r"Size: (\d+)", meta_line)

                if license_match:
                    current_metadata["license"] = license_match.group(1)
                if size_match:
                    current_metadata["size"] = size_match.group(1)

                # Get additional metadata from CSV
                if current_idx in metadata_dict:
                    current_metadata["line_mean"] = metadata_dict[current_idx][
                        "line_mean"
                    ]
                    current_metadata["line_max"] = metadata_dict[current_idx][
                        "line_max"
                    ]

                i += 1

            # Skip the closing separator
            if i < len(lines) and lines[i].strip() == "=" * 80:
                i += 1
        else:
            # Accumulate code lines
            if current_idx is not None:
                current_code.append(line)
            i += 1

# Don't forget the last sample
if current_idx is not None and current_code:
    content = "".join(current_code).strip()
    samples.append(
        {
            "repo_name": current_metadata["repo_name"],
            "path": current_metadata["path"],
            "copies": (
                int(metadata_dict[current_idx].get("copies", 0))
                if current_idx in metadata_dict
                else 0
            ),
            "size": int(current_metadata["size"]),
            "content": content,
            "license": current_metadata["license"],
            "hash": 0,
            "line_mean": float(current_metadata.get("line_mean", 0)),
            "line_max": int(current_metadata.get("line_max", 0)),
            "alpha_frac": 0.0,
            "autogenerated": False,
        }
    )

print(f"\n✓ Parsed {len(samples)} samples from text file")

# Create Arrow dataset
print("\nCreating Arrow dataset...")
dataset = Dataset.from_list(samples)

# Save to disk in Arrow format
print(f"Saving Arrow dataset to {output_dir}...")
os.makedirs(output_dir, exist_ok=True)
dataset.save_to_disk(output_dir)

print(f"\n✓ Arrow dataset saved to: {os.path.abspath(output_dir)}")
print(f"✓ Total samples: {len(dataset)}")
print(f"✓ Features: {list(dataset.features.keys())}")

# Verify the dataset
print("\nVerifying dataset...")
from datasets import load_from_disk

verified_dataset = load_from_disk(output_dir)
print(f"✓ Dataset loaded successfully")
print(f"✓ Sample count verified: {len(verified_dataset)}")

# Show first sample
print("\nFirst sample preview:")
sample = verified_dataset[0]
print(f"  Repo: {sample['repo_name']}")
print(f"  Path: {sample['path']}")
print(f"  License: {sample['license']}")
print(f"  Size: {sample['size']} bytes")
print(f"  Content preview: {sample['content'][:100]}...")

print("\nDone! You can now load the dataset with:")
print(f"  from datasets import load_from_disk")
print(f'  dataset = load_from_disk("{output_dir}")')
