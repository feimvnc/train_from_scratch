{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPCt4TrekG_X"
      },
      "outputs": [],
      "source": [
        "# Original source - https://gist.github.com/vukrosic/94dc965a22b0892042f44fed25918598\n",
        "\n",
        "# The notebook worked in google colab using torch==2.9.0 and python 3.12.12\n",
        "# The code does not work on MacOS x86 Intel because torch 2.4 not supported because\n",
        "# nn.RMSNorm is available in torch 2.4\n",
        "# Click Terminal at bottom, run \"nvidia smi\" for gpu info, \"watch -n 1 nvidia smi\" to see progress\n",
        "\n",
        "# After training 2000, 5000, 5000 iterations, you may get \"All GPU resources are used\", and\n",
        "# Ask you to switch to CPU, which was very slow\n",
        "# Wait a little, switch to CPU and then connect to GPU, then continue\n",
        "\n",
        "# Explanation and comments are removed for brevity, check original source.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn  # Neural network modules like Linear, Embedding, etc.\n",
        "import torch.nn.functional as F  # Functional interface for operations like cross_entropy, silu, etc.\n",
        "from torch.utils.data import Dataset, DataLoader  # Base class and utilities for loading datasets\n",
        "from torch.cuda.amp import autocast, GradScaler  # ðŸ”„ Automatic Mixed Precision (AMP) tools for faster/lower-memory training\n",
        "\n",
        "import math  # Standard math operations (e.g. sqrt, exp, cos)\n",
        "import random  # Python's random number utilities (used for seeding)\n",
        "import numpy as np  # Numerical computing library, used for random seeding and general array ops\n",
        "\n",
        "from datasets import load_dataset  # ðŸ§ Hugging Face Datasets library for streaming large datasets\n",
        "from tqdm import tqdm  # â³ Progress bar visualization library, great for loops\n",
        "\n",
        "import time  # âŒ› Timing utilities, measuring time\n",
        "from transformers import AutoTokenizer  # ðŸ¤— Load pretrained tokenizers from HuggingFace with one line\n",
        "\n",
        "from dataclasses import dataclass  # ðŸ§± Define simple classes for configs with less boilerplate\n",
        "from typing import List, Optional  # âœï¸ Type hints for better readability and tooling\n",
        "\n",
        "import warnings  # âš ï¸ Suppress or handle warnings\n",
        "import os  # ðŸ—‚ï¸ File system operations (creating folders, path checking, etc.)\n",
        "import pickle  # ðŸ’¾ Python object serialization (used to save/load preprocessed datasets)\n",
        "\n",
        "warnings.filterwarnings('ignore')  # Silences warnings for cleaner outputs during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uXrZOb2ukvEK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"ðŸŒ± Set all seeds to {seed}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "91T7S4UCk5wk"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    # Model architecture\n",
        "    d_model: int = 384\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 6\n",
        "    d_ff: int = 1536\n",
        "    batch_size: int = 24\n",
        "    max_steps: int = 2000\n",
        "\n",
        "    # Qwen3-like parameters\n",
        "    n_kv_heads: int = 4  # For Grouped-Query Attention\n",
        "    sliding_window: int = 4096  # Set a large default, effectively disabling it unless specified\n",
        "    attention_bias: bool = False  # Qwen3 often sets this to False\n",
        "    rms_norm_eps: float = 1e-6  # Epsilon for RMSNorm\n",
        "\n",
        "    # Training parameters\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    muon_lr: float = 0.001   # orig 0.01\n",
        "\n",
        "    # Data parameters\n",
        "    max_seq_len: int = 512\n",
        "    num_documents: int = 2000\n",
        "    max_tokens: int = 500000\n",
        "\n",
        "    # Evaluation\n",
        "    eval_every: int = 500\n",
        "    eval_steps: int = 100\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay: float = 0.1\n",
        "    dropout: float = 0.1\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    # Technical\n",
        "    use_amp: bool = True\n",
        "    vocab_size: Optional[int] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_k = self.d_model // self.n_heads\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_groups = self.n_heads // self.n_kv_heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lvQZOLSqlhiu"
      },
      "outputs": [],
      "source": [
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).\n",
        "    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim)\n",
        "    to (batch, num_attention_heads, seqlen, head_dim)\n",
        "    \"\"\"\n",
        "    # Extract dimensions from input tensor\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "\n",
        "    # Early return if no repetition is needed\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "\n",
        "    # Add a new dimension at index 2 (after num_key_value_heads) and expand\n",
        "    # Shape transformation:\n",
        "    # (batch, num_key_value_heads, slen, head_dim)\n",
        "    # -> (batch, num_key_value_heads, 1, slen, head_dim) [via None indexing]\n",
        "    # -> (batch, num_key_value_heads, n_rep, slen, head_dim) [via expand]\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "\n",
        "    # Flatten the num_key_value_heads and n_rep dimensions together\n",
        "    # Final shape: (batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "    # This effectively repeats each key/value head n_rep times\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss0dci0flmYJ",
        "outputId": "0f847ab0-953f-4e65-e985-d03fe656c017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Tensor Expansion and Repetition Exercises\n",
            "==================================================\n",
            "\n",
            "ðŸ“ Exercise 1: Understanding Tensor Shapes\n",
            "----------------------------------------\n",
            "1D tensor: tensor([1, 2, 3])\n",
            "Shape: torch.Size([3])\n",
            "2D tensor:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Shape: torch.Size([2, 3])\n",
            "3D tensor:\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "Shape: torch.Size([2, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Tensor Expansion and Repetition Practice Exercises\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(\"ðŸš€ Tensor Expansion and Repetition Exercises\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# =============================================================================\n",
        "# EXERCISE 1: Basic Tensor Creation and Shapes\n",
        "# =============================================================================\n",
        "print(\"\\nðŸ“ Exercise 1: Understanding Tensor Shapes\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create simple tensors and understand their shapes\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(f\"1D tensor: {x}\")\n",
        "print(f\"Shape: {x.shape}\")\n",
        "\n",
        "y = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(f\"2D tensor:\\n{y}\")\n",
        "print(f\"Shape: {y.shape}\")\n",
        "\n",
        "z = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "print(f\"3D tensor:\\n{z}\")\n",
        "print(f\"Shape: {z.shape}\")\n",
        "\n",
        "# TODO: Create a 4D tensor of shape (2, 3, 4, 5) filled with ones\n",
        "# Your code here:\n",
        "# tensor_4d = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbgiOIOPlrCx",
        "outputId": "4f030b36-4cf0-4c09-d151-99be2838c79a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 2: Adding Dimensions with None\n",
            "----------------------------------------\n",
            "Original: torch.Size([4]) -> tensor([1, 2, 3, 4])\n",
            "Add dim at 0: torch.Size([1, 4]) -> tensor([[1, 2, 3, 4]])\n",
            "Add dim at 1: torch.Size([4, 1]) -> tensor([[1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4]])\n",
            "Add dim at end: torch.Size([4, 1]) -> tensor([[1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4]])\n",
            "Multiple dims: torch.Size([1, 4, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 2: Understanding None Indexing (Adding Dimensions)\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 2: Adding Dimensions with None\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Start with a 1D tensor\n",
        "a = torch.tensor([1, 2, 3, 4])\n",
        "print(f\"Original: {a.shape} -> {a}\")\n",
        "\n",
        "# Add dimension at different positions\n",
        "a_new_dim0 = a[None, :]  # or a.unsqueeze(0)\n",
        "print(f\"Add dim at 0: {a_new_dim0.shape} -> {a_new_dim0}\")\n",
        "\n",
        "a_new_dim1 = a[:, None]  # or a.unsqueeze(1)\n",
        "print(f\"Add dim at 1: {a_new_dim1.shape} -> {a_new_dim1}\")\n",
        "\n",
        "a_new_dim_end = a[..., None]  # or a.unsqueeze(-1)\n",
        "print(f\"Add dim at end: {a_new_dim_end.shape} -> {a_new_dim_end}\")\n",
        "\n",
        "# Multiple dimensions\n",
        "a_multi = a[None, :, None, None]\n",
        "print(f\"Multiple dims: {a_multi.shape}\")\n",
        "\n",
        "# TODO: Take tensor [1, 2, 3] and make it shape (1, 3, 1, 1)\n",
        "# Your code here:\n",
        "# result = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3IYJhuhltUS",
        "outputId": "49803839-1399-479f-db34-54213966b499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 3: Understanding expand()\n",
            "----------------------------------------\n",
            "Original: torch.Size([1, 3]) -> tensor([[1, 2, 3]])\n",
            "Expanded: torch.Size([4, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [1, 2, 3],\n",
            "        [1, 2, 3],\n",
            "        [1, 2, 3]])\n",
            "\n",
            "Original c: torch.Size([3, 1])\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "Expanded c: torch.Size([3, 5])\n",
            "tensor([[1, 1, 1, 1, 1],\n",
            "        [2, 2, 2, 2, 2],\n",
            "        [3, 3, 3, 3, 3]])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 3: Basic expand() Operation\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 3: Understanding expand()\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# expand() creates a view with repeated elements (no memory copy!)\n",
        "b = torch.tensor([[1, 2, 3]])  # Shape: (1, 3)\n",
        "print(f\"Original: {b.shape} -> {b}\")\n",
        "\n",
        "# Expand the first dimension\n",
        "b_expanded = b.expand(4, 3)  # Repeat the row 4 times\n",
        "print(f\"Expanded: {b_expanded.shape}\")\n",
        "print(b_expanded)\n",
        "\n",
        "# Expand with -1 (keep original size)\n",
        "c = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\n",
        "print(f\"\\nOriginal c: {c.shape}\")\n",
        "print(c)\n",
        "\n",
        "c_expanded = c.expand(-1, 5)  # Keep dim 0, expand dim 1 to 5\n",
        "print(f\"Expanded c: {c_expanded.shape}\")\n",
        "print(c_expanded)\n",
        "\n",
        "# TODO: Create tensor [[1, 2]] and expand it to shape (3, 4)\n",
        "# Your code here:\n",
        "# d = ?\n",
        "# d_expanded = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vEqm3Xwlv80",
        "outputId": "f2769ee9-8edd-4cf3-9972-6fe5839a840b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 4: Different Repetition Methods\n",
            "----------------------------------------\n",
            "Original: tensor([1, 2, 3])\n",
            "repeat(2): tensor([1, 2, 3, 1, 2, 3])\n",
            "repeat(2, 1) on [1,2,3]: shape torch.Size([2, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [1, 2, 3]])\n",
            "expand(3, -1): shape torch.Size([3, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [1, 2, 3],\n",
            "        [1, 2, 3]])\n",
            "repeat_interleave(2): tensor([1, 1, 2, 2, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 4: repeat() vs expand() vs repeat_interleave()\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 4: Different Repetition Methods\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "original = torch.tensor([1, 2, 3])\n",
        "print(f\"Original: {original}\")\n",
        "\n",
        "# Method 1: repeat() - actually copies data\n",
        "repeated = original.repeat(2)  # Repeat entire tensor 2 times\n",
        "print(f\"repeat(2): {repeated}\")\n",
        "\n",
        "repeated_2d = original.repeat(2, 1)  # 2D repetition\n",
        "print(f\"repeat(2, 1) on [1,2,3]: shape {repeated_2d.shape}\")\n",
        "print(repeated_2d)\n",
        "\n",
        "# Method 2: expand() - creates view (memory efficient)\n",
        "original_2d = original.unsqueeze(0)  # Make it (1, 3)\n",
        "expanded = original_2d.expand(3, -1)\n",
        "print(f\"expand(3, -1): shape {expanded.shape}\")\n",
        "print(expanded)\n",
        "\n",
        "# Method 3: repeat_interleave() - repeats each element\n",
        "interleaved = torch.repeat_interleave(original, 2)\n",
        "print(f\"repeat_interleave(2): {interleaved}\")\n",
        "\n",
        "# TODO: What's the difference between these results?\n",
        "# torch.tensor([1, 2]).repeat(3) vs torch.repeat_interleave(torch.tensor([1, 2]), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irC97-oaly1f",
        "outputId": "c883cce7-ffa5-4c88-ee62-411964285faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 5: 3D Tensor Manipulations\n",
            "----------------------------------------\n",
            "3D tensor shape: torch.Size([2, 3, 4])\n",
            "3D tensor:\n",
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n",
            "\n",
            "After adding dim: torch.Size([2, 3, 1, 4])\n",
            "After expand: torch.Size([2, 3, 5, 4])\n",
            "First batch, first head:\n",
            "tensor([[0, 1, 2, 3],\n",
            "        [0, 1, 2, 3],\n",
            "        [0, 1, 2, 3],\n",
            "        [0, 1, 2, 3],\n",
            "        [0, 1, 2, 3]])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 5: Working with 3D Tensors\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 5: 3D Tensor Manipulations\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create a 3D tensor: (batch=2, heads=3, features=4)\n",
        "tensor_3d = torch.arange(24).reshape(2, 3, 4)\n",
        "print(f\"3D tensor shape: {tensor_3d.shape}\")\n",
        "print(f\"3D tensor:\\n{tensor_3d}\")\n",
        "\n",
        "# Add a dimension in the middle\n",
        "tensor_4d = tensor_3d[:, :, None, :]  # Shape: (2, 3, 1, 4)\n",
        "print(f\"\\nAfter adding dim: {tensor_4d.shape}\")\n",
        "\n",
        "# Expand the new dimension\n",
        "tensor_expanded = tensor_4d.expand(2, 3, 5, 4)  # Shape: (2, 3, 5, 4)\n",
        "print(f\"After expand: {tensor_expanded.shape}\")\n",
        "print(f\"First batch, first head:\\n{tensor_expanded[0, 0]}\")\n",
        "\n",
        "# TODO: Take the expanded tensor and reshape it to merge dimensions 1 and 2\n",
        "# Target shape: (2, 15, 4)  # 3 * 5 = 15\n",
        "# Your code here:\n",
        "# merged = ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JJee4gcl1K-",
        "outputId": "36ef168b-273f-4050-bd90-15396f3a592f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 6: Building Up to repeat_kv\n",
            "----------------------------------------\n",
            "KV tensor shape: torch.Size([2, 3, 4, 2])\n",
            "KV tensor (batch 0, head 0):\n",
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7]])\n",
            "\n",
            "Step 1 - Add dimension: torch.Size([2, 3, 1, 4, 2])\n",
            "Step 2 - Expand: torch.Size([2, 3, 2, 4, 2])\n",
            "Step 3 - Final: torch.Size([2, 6, 4, 2])\n",
            "\n",
            "Original head 0:\n",
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7]])\n",
            "Repeated head 0 (position 0):\n",
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7]])\n",
            "Repeated head 0 (position 1):\n",
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7]])\n",
            "Original head 1:\n",
            "tensor([[ 8,  9],\n",
            "        [10, 11],\n",
            "        [12, 13],\n",
            "        [14, 15]])\n",
            "Repeated head 1 (position 2):\n",
            "tensor([[ 8,  9],\n",
            "        [10, 11],\n",
            "        [12, 13],\n",
            "        [14, 15]])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 6: Simulating the repeat_kv Pattern\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 6: Building Up to repeat_kv\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Simulate key/value heads that need to be repeated\n",
        "# Shape: (batch, num_kv_heads, seq_len, head_dim)\n",
        "kv_tensor = torch.arange(48).reshape(2, 3, 4, 2)\n",
        "print(f\"KV tensor shape: {kv_tensor.shape}\")\n",
        "print(f\"KV tensor (batch 0, head 0):\\n{kv_tensor[0, 0]}\")\n",
        "\n",
        "n_rep = 2  # Each KV head needs to be repeated 2 times\n",
        "\n",
        "# Step 1: Add dimension for repetition\n",
        "step1 = kv_tensor[:, :, None, :, :]  # Shape: (2, 3, 1, 4, 2)\n",
        "print(f\"\\nStep 1 - Add dimension: {step1.shape}\")\n",
        "\n",
        "# Step 2: Expand the new dimension\n",
        "step2 = step1.expand(2, 3, n_rep, 4, 2)  # Shape: (2, 3, 2, 4, 2)\n",
        "print(f\"Step 2 - Expand: {step2.shape}\")\n",
        "\n",
        "# Step 3: Reshape to merge heads\n",
        "final = step2.reshape(2, 3 * n_rep, 4, 2)  # Shape: (2, 6, 4, 2)\n",
        "print(f\"Step 3 - Final: {final.shape}\")\n",
        "\n",
        "# Verify: each original head should appear n_rep times\n",
        "print(f\"\\nOriginal head 0:\\n{kv_tensor[0, 0]}\")\n",
        "print(f\"Repeated head 0 (position 0):\\n{final[0, 0]}\")\n",
        "print(f\"Repeated head 0 (position 1):\\n{final[0, 1]}\")\n",
        "print(f\"Original head 1:\\n{kv_tensor[0, 1]}\")\n",
        "print(f\"Repeated head 1 (position 2):\\n{final[0, 2]}\")\n",
        "\n",
        "# TODO: Verify that final[0, 0] equals final[0, 1] (same repeated head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZukWi3E3l4P-",
        "outputId": "3edbde5c-4abd-4fc8-a432-7c090e6a0397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 7: Implement repeat_kv Yourself\n",
            "----------------------------------------\n",
            "Test input shape: torch.Size([1, 3, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 7: Complete repeat_kv Implementation Practice\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 7: Implement repeat_kv Yourself\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "def my_repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    TODO: Implement this function!\n",
        "    Input: (batch, num_key_value_heads, seqlen, head_dim)\n",
        "    Output: (batch, num_key_value_heads * n_rep, seqlen, head_dim)\n",
        "    \"\"\"\n",
        "    # Get dimensions\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "\n",
        "    # Handle n_rep = 1 case\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "\n",
        "    # TODO: Your implementation here\n",
        "    # Step 1: Add dimension\n",
        "    # Step 2: Expand\n",
        "    # Step 3: Reshape\n",
        "    # return ?\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "test_tensor = torch.arange(24).reshape(1, 3, 4, 2)\n",
        "print(f\"Test input shape: {test_tensor.shape}\")\n",
        "\n",
        "# TODO: Uncomment when you implement the function\n",
        "# result = my_repeat_kv(test_tensor, 2)\n",
        "# print(f\"Result shape: {result.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_CYVgLul81w"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-to9xrkl6yG",
        "outputId": "2347f16c-3f11-4251-883c-71a71d6f2d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 8: Advanced Repetition Patterns\n",
            "----------------------------------------\n",
            "Base tensor:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Pattern 1 - repeat(3, 2):\n",
            "tensor([[1, 2, 1, 2],\n",
            "        [3, 4, 3, 4],\n",
            "        [1, 2, 1, 2],\n",
            "        [3, 4, 3, 4],\n",
            "        [1, 2, 1, 2],\n",
            "        [3, 4, 3, 4]])\n",
            "Pattern 2 - repeat_interleave along dim 0:\n",
            "tensor([[1, 2],\n",
            "        [1, 2],\n",
            "        [3, 4],\n",
            "        [3, 4]])\n",
            "Pattern 3 - repeat_interleave along dim 1:\n",
            "tensor([[1, 1, 2, 2],\n",
            "        [3, 3, 4, 4]])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 8: Advanced Patterns\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 8: Advanced Repetition Patterns\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Pattern 1: Repeat different amounts for different dimensions\n",
        "base = torch.tensor([[1, 2], [3, 4]])  # Shape: (2, 2)\n",
        "print(f\"Base tensor:\\n{base}\")\n",
        "\n",
        "# Repeat 3 times along dim 0, 2 times along dim 1\n",
        "pattern1 = base.repeat(3, 2)\n",
        "print(f\"Pattern 1 - repeat(3, 2):\\n{pattern1}\")\n",
        "\n",
        "# Pattern 2: Using repeat_interleave along specific dimensions\n",
        "pattern2 = torch.repeat_interleave(base, 2, dim=0)\n",
        "print(f\"Pattern 2 - repeat_interleave along dim 0:\\n{pattern2}\")\n",
        "\n",
        "pattern3 = torch.repeat_interleave(base, 2, dim=1)\n",
        "print(f\"Pattern 3 - repeat_interleave along dim 1:\\n{pattern3}\")\n",
        "\n",
        "# TODO: Create a pattern where you repeat_interleave along dim 0 with repeats [1, 3]\n",
        "# (first row once, second row three times)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzWh9fhKl9qV",
        "outputId": "b70ebd0b-daa6-462a-dd94-344105ef5bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 9: Memory Usage Comparison\n",
            "----------------------------------------\n",
            "Original tensor memory: 20000 bytes\n",
            "Expanded tensor shares memory: True\n",
            "Repeated tensor shares memory: False\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 9: Memory Efficiency Test\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 9: Memory Usage Comparison\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "large_tensor = torch.randn(100, 50)\n",
        "print(f\"Original tensor memory: {large_tensor.numel() * large_tensor.element_size()} bytes\")\n",
        "\n",
        "# Method 1: expand (memory efficient - creates view)\n",
        "expanded = large_tensor.unsqueeze(0).expand(10, -1, -1)\n",
        "print(f\"Expanded tensor shares memory: {expanded.storage().data_ptr() == large_tensor.storage().data_ptr()}\")\n",
        "\n",
        "# Method 2: repeat (creates copy)\n",
        "repeated = large_tensor.repeat(10, 1)\n",
        "print(f\"Repeated tensor shares memory: {repeated.storage().data_ptr() == large_tensor.storage().data_ptr()}\")\n",
        "\n",
        "# TODO: What happens if you modify the original tensor? Will the expanded version change too?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHRadpnmmAHg",
        "outputId": "c2c74ab0-1330-4551-c8b1-e3fac66b5f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸ“ Exercise 10: Attention Mechanism Context\n",
            "----------------------------------------\n",
            "Scenario: 12 query heads, 4 KV heads\n",
            "Need to repeat each KV head 3 times\n",
            "Original keys shape: torch.Size([2, 4, 8, 64])\n",
            "Original values shape: torch.Size([2, 4, 8, 64])\n",
            "Repeated keys shape: torch.Size([2, 12, 8, 64])\n",
            "Repeated values shape: torch.Size([2, 12, 8, 64])\n",
            "Success! KV heads now match query heads: True\n",
            "\n",
            "ðŸŽ‰ Exercises Complete!\n",
            "Next steps:\n",
            "1. Try modifying the dimensions and see how shapes change\n",
            "2. Experiment with different n_rep values\n",
            "3. Compare memory usage between expand() and repeat()\n",
            "4. Implement your own version of repeat_kv from scratch!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EXERCISE 10: Real-World Application - Attention Mechanism\n",
        "# =============================================================================\n",
        "print(\"\\n\\nðŸ“ Exercise 10: Attention Mechanism Context\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Simulate a real attention scenario\n",
        "batch_size = 2\n",
        "seq_len = 8\n",
        "num_query_heads = 12\n",
        "num_kv_heads = 4  # Fewer KV heads than query heads (Grouped Query Attention)\n",
        "head_dim = 64\n",
        "\n",
        "print(f\"Scenario: {num_query_heads} query heads, {num_kv_heads} KV heads\")\n",
        "print(f\"Need to repeat each KV head {num_query_heads // num_kv_heads} times\")\n",
        "\n",
        "# Create mock key and value tensors\n",
        "keys = torch.randn(batch_size, num_kv_heads, seq_len, head_dim)\n",
        "values = torch.randn(batch_size, num_kv_heads, seq_len, head_dim)\n",
        "\n",
        "print(f\"Original keys shape: {keys.shape}\")\n",
        "print(f\"Original values shape: {values.shape}\")\n",
        "\n",
        "# Apply repeat_kv to match query heads\n",
        "n_rep = num_query_heads // num_kv_heads\n",
        "\n",
        "def repeat_kv_solution(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "keys_repeated = repeat_kv_solution(keys, n_rep)\n",
        "values_repeated = repeat_kv_solution(values, n_rep)\n",
        "\n",
        "print(f\"Repeated keys shape: {keys_repeated.shape}\")\n",
        "print(f\"Repeated values shape: {values_repeated.shape}\")\n",
        "\n",
        "# TODO: Verify that we now have the same number of heads for keys, values, and queries\n",
        "print(f\"Success! KV heads now match query heads: {keys_repeated.shape[1] == num_query_heads}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Exercises Complete!\")\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Try modifying the dimensions and see how shapes change\")\n",
        "print(\"2. Experiment with different n_rep values\")\n",
        "print(\"3. Compare memory usage between expand() and repeat()\")\n",
        "print(\"4. Implement your own version of repeat_kv from scratch!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "7QYvU_UymGjD"
      },
      "outputs": [],
      "source": [
        "@torch.compile\n",
        "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n",
        "    \"\"\"Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\"\"\"\n",
        "    assert G.ndim >= 2\n",
        "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
        "    X = G.bfloat16()\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.mT\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    return X\n",
        "\n",
        "class Muon(torch.optim.Optimizer):\n",
        "    \"\"\"Muon - MomentUm Orthogonalized by Newton-schulz\"\"\"\n",
        "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
        "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                g = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                # Initialize momentum buffer if first time\n",
        "                if \"momentum_buffer\" not in state:\n",
        "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
        "\n",
        "                buf = state[\"momentum_buffer\"]\n",
        "                # Update momentum buffer: buf = momentum * buf + (1-momentum) * grad\n",
        "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
        "                # Apply Nesterov momentum if enabled, otherwise use standard momentum\n",
        "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
        "                # Apply zero-power normalization via Newton-Schulz iterations (make it close to orthonormal)\n",
        "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
        "                # Update parameters with adaptive scaling based on parameter shape\n",
        "                p.add_(g.view_as(p), alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)\n",
        "                # Updates parameters with an adaptive learning rate that scales based on the parameter tensor's aspect ratio (height/width). For matrices where height > width, it increases the effective learning rate by âˆš(height/width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nqSiN0VTn6_G"
      },
      "outputs": [],
      "source": [
        "def load_and_cache_data(config: ModelConfig, cache_dir: str = \"data_cache\"):\n",
        "    \"\"\"Load and cache tokenized data to avoid reprocessing\"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_file = f\"{cache_dir}/tokenized_data_{config.num_documents}_{config.max_tokens}.pkl\"\n",
        "\n",
        "    # Check if cached data exists\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"ðŸ“¦ Loading cached data from {cache_file}\")\n",
        "        with open(cache_file, 'rb') as f:\n",
        "            cached_data = pickle.load(f)\n",
        "\n",
        "        texts = cached_data['texts']\n",
        "        tokenizer = cached_data['tokenizer']\n",
        "        tokens = cached_data['tokens']\n",
        "        config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "        print(f\"âœ… Loaded {len(texts)} documents, {len(tokens):,} tokens from cache\")\n",
        "        return texts, tokenizer, tokens\n",
        "\n",
        "    print(f\"ðŸ”„ Processing new data (will cache for future use)\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True)\n",
        "\n",
        "    texts = []\n",
        "    for i, item in enumerate(dataset):\n",
        "        if i >= config.num_documents:\n",
        "            break\n",
        "        texts.append(item[\"text\"][:3000])\n",
        "\n",
        "    print(f\"Loaded {len(texts)} documents\")\n",
        "\n",
        "    # Tokenize\n",
        "    print(\"Tokenizing texts...\")\n",
        "    all_tokens = []\n",
        "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
        "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    tokens = all_tokens[:config.max_tokens]\n",
        "    print(f\"Using {len(tokens):,} tokens\")\n",
        "    config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Cache the processed data\n",
        "    cached_data = {'texts': texts, 'tokenizer': tokenizer, 'tokens': tokens}\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump(cached_data, f)\n",
        "\n",
        "    print(f\"ðŸ’¾ Cached data to {cache_file}\")\n",
        "    return texts, tokenizer, tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ameSz2OfmS5F"
      },
      "outputs": [],
      "source": [
        "class TextTokenDataset(Dataset):\n",
        "    def __init__(self, tokens: List[int], seq_len: int = 512):\n",
        "        self.tokens = tokens\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.tokens) - self.seq_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RmaC0YtBn-oD"
      },
      "outputs": [],
      "source": [
        "class Rotary(nn.Module):\n",
        "    def __init__(self, dim: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        angular_freq = (1 / 10000) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)\n",
        "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])\n",
        "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
        "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
        "        self.register_buffer('cos', theta.cos(), persistent=False)\n",
        "        self.register_buffer('sin', theta.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, x_BTHD: torch.Tensor):\n",
        "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
        "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
        "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
        "        y1 = x1 * cos + x2 * sin\n",
        "        y2 = x1 * (-sin) + x2 * cos\n",
        "        return torch.cat((y1, y2), 3).type_as(x_BTHD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NvRemcJGoScv"
      },
      "outputs": [],
      "source": [
        "class Qwen3Attention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.n_heads = config.n_heads\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_kv_groups = config.n_kv_groups\n",
        "        self.d_k = config.d_k\n",
        "\n",
        "        # Separate linear layers for Q, K, V\n",
        "        self.q_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "        # QK-Normalization layers\n",
        "        # Practice RMSNorm 1 on 1 with ChatGPT - https://chatgpt.com/share/68945c86-2dd4-8002-b017-725caab0c107\n",
        "        self.q_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
        "        self.k_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
        "\n",
        "        self.rotary = Rotary(self.d_k, config.max_seq_len)\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # 1. Project Q, K, V separately\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # 2. Reshape into heads\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
        "        k = k.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
        "        v = v.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
        "\n",
        "        # 3. Apply QK-Norm\n",
        "        q = self.q_norm(q)\n",
        "        k = self.k_norm(k)\n",
        "\n",
        "        # 4. Apply RoPE\n",
        "        # Transpose to (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k) for rotary\n",
        "        q = self.rotary(q.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "        k = self.rotary(k.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Transpose for attention: (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)\n",
        "        Q = q.transpose(1, 2)\n",
        "        K = k.transpose(1, 2)\n",
        "        V = v.transpose(1, 2)\n",
        "\n",
        "        # 5. Repeat K and V heads for GQA\n",
        "        K = repeat_kv(K, self.n_kv_groups)\n",
        "        V = repeat_kv(V, self.n_kv_groups)\n",
        "\n",
        "        # 6. Scaled Dot-Product Attention\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            Q, K, V, is_causal=True, dropout_p=self.dropout if self.training else 0.0\n",
        "        )\n",
        "\n",
        "        # 7. Reshape and final projection\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.w_o(attn_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "JL8IyDiEoVLj"
      },
      "outputs": [],
      "source": [
        "class SwiGLUFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implementation of the SwiGLU activation function\n",
        "        # F.silu is the Swish activation function\n",
        "        activated_x = F.silu(self.gate_proj(x)) * self.up_proj(x)\n",
        "        return self.down_proj(self.dropout(activated_x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8Qrhp0DIoXUx"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):  # Pass the entire config object\n",
        "        super().__init__()\n",
        "        self.attention = Qwen3Attention(config)\n",
        "        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n",
        "        self.norm1 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.norm2 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.attention(self.norm1(x))\n",
        "        x = x + self.dropout(attn_out)\n",
        "        ff_out = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zlVtEYsUqpHU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "giT3uWtcoZMe"
      },
      "outputs": [],
      "source": [
        "class MinimalLLM(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.output_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Tie weights\n",
        "        # This ties the output layer (`lm_head`) weights to the input token embedding weights so the model shares parameters between input and output, reducing memory and improving generalization.\n",
        "        # https://chatgpt.com/share/6894683e-ba44-8002-ae82-e42b4afc9d98\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_embedding.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x) * math.sqrt(self.config.d_model)\n",
        "        x = self.position_dropout(x)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.output_dropout(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4_ioa8csocla"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: ModelConfig):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation (saves memory and computation)\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            # Stop evaluation after specified number of steps to limit eval time\n",
        "            if i >= config.eval_steps:\n",
        "                break\n",
        "\n",
        "            # Move input sequences (x) and target sequences (y) to GPU/device\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Use automatic mixed precision if enabled (faster training with minimal accuracy loss)\n",
        "            with autocast(enabled=config.use_amp):\n",
        "                # Forward pass: get model predictions (logits) for input sequence\n",
        "                logits = model(x)\n",
        "\n",
        "                # Calculate cross-entropy loss between predictions and targets\n",
        "                # Reshape to (batch_size * seq_len, vocab_size) and (batch_size * seq_len,)\n",
        "                # for proper cross-entropy computation across all token positions\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "\n",
        "            # Accumulate total loss weighted by number of tokens in this batch\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            # Keep track of total number of tokens processed\n",
        "            total_tokens += y.numel()\n",
        "\n",
        "            # Get predicted token IDs by taking argmax over vocabulary dimension\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            # Count correct predictions for accuracy calculation\n",
        "            total_correct += (predictions == y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    accuracy = total_correct / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 20))\n",
        "\n",
        "    model.train()\n",
        "    return {'val_loss': avg_loss, 'val_accuracy': accuracy, 'val_perplexity': perplexity}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6xBTeUXoh3c"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "WJATOy6Kofja"
      },
      "outputs": [],
      "source": [
        "def setup_muon_optimizer(model: nn.Module, config: ModelConfig):\n",
        "    \"\"\"Setup Muon optimizer with hybrid approach\"\"\"\n",
        "    muon_params = []\n",
        "    adamw_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if (param.ndim == 2 and\n",
        "            'token_embedding' not in name and\n",
        "            'norm' not in name and\n",
        "            param.requires_grad):\n",
        "            muon_params.append(param)\n",
        "        else:\n",
        "            adamw_params.append(param)\n",
        "\n",
        "    print(f\"  Muon parameters: {sum(p.numel() for p in muon_params):,}\")\n",
        "    print(f\"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}\")\n",
        "\n",
        "    muon_optimizer = Muon(muon_params, lr=config.muon_lr, momentum=0.95)\n",
        "    adamw_optimizer = torch.optim.AdamW(adamw_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay)\n",
        "\n",
        "    return [muon_optimizer, adamw_optimizer]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "m-qVuxt_ojjI"
      },
      "outputs": [],
      "source": [
        "def train_model(config: ModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n",
        "    \"\"\"Train the model with Muon optimizer\"\"\"\n",
        "    print(f\"\\nðŸš€ Training Small model with Muon optimizer\")\n",
        "\n",
        "    # Initialize model\n",
        "    set_seed(42)\n",
        "    model = MinimalLLM(config)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  ðŸ“Š Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Setup optimizers\n",
        "    optimizers = setup_muon_optimizer(model, config)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    schedulers = []\n",
        "    for optimizer in optimizers:\n",
        "        warmup_steps = config.max_steps // 20\n",
        "        def lr_lambda(step):\n",
        "            if step < warmup_steps:\n",
        "                return step / warmup_steps\n",
        "            else:\n",
        "                progress = (step - warmup_steps) / (config.max_steps - warmup_steps)\n",
        "                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "        schedulers.append(scheduler)\n",
        "\n",
        "    scaler = GradScaler() if config.use_amp else None\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    step = 0\n",
        "    start_time = time.time()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    pbar = tqdm(total=config.max_steps, desc=\"Training\")\n",
        "\n",
        "    while step < config.max_steps:\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            if step >= config.max_steps:\n",
        "                break\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass with gradient accumulation\n",
        "            if config.use_amp:\n",
        "                with autocast():\n",
        "                    logits = model(x)\n",
        "                    loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                    loss = loss / config.gradient_accumulation_steps\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                loss = loss / config.gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "            # Optimizer step after accumulation\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                if config.use_amp:\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.step(optimizer)\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "\n",
        "            # Logging\n",
        "            if step % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    predictions = logits.argmax(dim=-1)\n",
        "                    accuracy = (predictions == y).float().mean().item()\n",
        "                    current_loss = loss.item() * config.gradient_accumulation_steps\n",
        "                    perplexity = math.exp(min(current_loss, 20))\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{current_loss:.4f}',\n",
        "                    'acc': f'{accuracy:.3f}',\n",
        "                    'ppl': f'{perplexity:.1f}',\n",
        "                    'lr': f'{optimizers[0].param_groups[0][\"lr\"]:.2e}'\n",
        "                })\n",
        "\n",
        "            # Evaluation\n",
        "            if step % config.eval_every == 0 and step > 0:\n",
        "                eval_metrics = evaluate_model(model, val_loader, config)\n",
        "                print(f\"\\nStep {step}: Val Loss: {eval_metrics['val_loss']:.4f}, \"\n",
        "                      f\"Val Acc: {eval_metrics['val_accuracy']:.4f}, \"\n",
        "                      f\"Val PPL: {eval_metrics['val_perplexity']:.2f}\")\n",
        "\n",
        "                if eval_metrics['val_loss'] < best_val_loss:\n",
        "                    best_val_loss = eval_metrics['val_loss']\n",
        "                    # Save best model\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'config': config,\n",
        "                        'step': step,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'final_metrics': eval_metrics\n",
        "                    }, 'best_model.pt')\n",
        "                    print(f\"ðŸ’¾ Saved best model with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            step += 1\n",
        "            if step % 10 == 0:\n",
        "                pbar.update(10)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"  â±ï¸ Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "    # Final evaluation\n",
        "    final_eval = evaluate_model(model, val_loader, config)\n",
        "    print(f\"  ðŸ“Š Final - Loss: {final_eval['val_loss']:.4f}, \"\n",
        "          f\"Acc: {final_eval['val_accuracy']:.4f}, PPL: {final_eval['val_perplexity']:.2f}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'step': step,\n",
        "        'final_metrics': final_eval\n",
        "    }, 'final_model.pt')\n",
        "    print(f\"ðŸ’¾ Saved final model to final_model.pt\")\n",
        "\n",
        "    return model, final_eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaBMCN7Yop4H",
        "outputId": "7d916bf8-6bd0-45c5-ad68-25f40b68d260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Device: CUDA\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n",
            "ðŸŒ± Set all seeds to 42\n",
            "\n",
            "ðŸ“‹ Model Configuration:\n",
            "   Architecture: 384d, 6L, 8H, 1536ff\n",
            "   Training: 2000 steps, batch size 24\n",
            "   Data: 500,000 tokens, seq_len 512\n",
            "ðŸ“¦ Loading cached data from data_cache/tokenized_data_2000_500000.pkl\n",
            "âœ… Loaded 2000 documents, 500,000 tokens from cache\n",
            "ðŸ“Š Dataset: 449540 train, 49948 val samples\n",
            "\n",
            "ðŸš€ Training Small model with Muon optimizer\n",
            "ðŸŒ± Set all seeds to 42\n",
            "  ðŸ“Š Total parameters: 32,150,976\n",
            "  Muon parameters: 13,271,040\n",
            "  AdamW parameters: 18,879,936\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  25%|â–ˆâ–ˆâ–Œ       | 500/2000 [02:46<08:26,  2.96it/s, loss=8.0689, acc=0.073, ppl=3193.6, lr=1.00e-03]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 500: Val Loss: 7.9587, Val Acc: 0.0754, Val PPL: 2860.45\n",
            "ðŸ’¾ Saved best model with val_loss: 7.9587\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1000/2000 [05:44<05:35,  2.98it/s, loss=6.5456, acc=0.165, ppl=696.2, lr=9.86e-04]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 1000: Val Loss: 6.4605, Val Acc: 0.1710, Val PPL: 639.41\n",
            "ðŸ’¾ Saved best model with val_loss: 6.4605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1500/2000 [08:41<02:48,  2.98it/s, loss=5.8510, acc=0.201, ppl=347.6, lr=9.54e-04]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 1500: Val Loss: 5.7175, Val Acc: 0.2063, Val PPL: 304.14\n",
            "ðŸ’¾ Saved best model with val_loss: 5.7175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [11:39<00:00,  2.86it/s, loss=5.3792, acc=0.209, ppl=216.8, lr=9.06e-04]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  â±ï¸ Training completed in 699.3 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ðŸ“Š Final - Loss: 5.2910, Acc: 0.2304, PPL: 198.55\n",
            "ðŸ’¾ Saved final model to final_model.pt\n",
            "\n",
            "ðŸŽ‰ TRAINING COMPLETED!\n",
            "â±ï¸ Total time: 11.9 minutes\n",
            "ðŸ† Final Results:\n",
            "   Validation Loss: 5.2910\n",
            "   Validation Accuracy: 0.2304\n",
            "   Validation Perplexity: 198.55\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check system\n",
        "    print(f\"ðŸ” Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(42)\n",
        "\n",
        "    # Create config for Small model\n",
        "    config = ModelConfig()\n",
        "    print(f\"\\nðŸ“‹ Model Configuration:\")\n",
        "    print(f\"   Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "    print(f\"   Training: {config.max_steps} steps, batch size {config.batch_size}\")\n",
        "    print(f\"   Data: {config.max_tokens:,} tokens, seq_len {config.max_seq_len}\")\n",
        "\n",
        "    # Load data\n",
        "    texts, tokenizer, tokens = load_and_cache_data(config)\n",
        "    dataset = TextTokenDataset(tokens, config.max_seq_len)\n",
        "\n",
        "    # Train/val split\n",
        "    val_size = len(dataset) // 10\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"ðŸ“Š Dataset: {len(train_dataset)} train, {len(val_dataset)} val samples\")\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    model, final_metrics = train_model(config, train_loader, val_loader)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ TRAINING COMPLETED!\")\n",
        "    print(f\"â±ï¸ Total time: {total_time/60:.1f} minutes\")\n",
        "    print(f\"ðŸ† Final Results:\")\n",
        "    print(f\"   Validation Loss: {final_metrics['val_loss']:.4f}\")\n",
        "    print(f\"   Validation Accuracy: {final_metrics['val_accuracy']:.4f}\")\n",
        "    print(f\"   Validation Perplexity: {final_metrics['val_perplexity']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "EF36I2j4pI41"
      },
      "outputs": [],
      "source": [
        "def load_trained_model(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"Load a trained model from checkpoint\"\"\"\n",
        "    print(f\" Loading model from {model_path}\")\n",
        "\n",
        "    # Add ModelConfig to safe globals for PyTorch 2.6+\n",
        "    from torch.serialization import add_safe_globals\n",
        "    add_safe_globals([ModelConfig])\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location='cpu')\n",
        "        config = checkpoint['config']\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error loading with weights_only=True, trying with weights_only=False...\")\n",
        "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
        "        config = checkpoint['config']\n",
        "\n",
        "    # Create model with same config\n",
        "    model = MinimalLLM(config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"âœ… Model loaded successfully\")\n",
        "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"   Device: {device}\")\n",
        "\n",
        "    return model, config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyGZOUVZpJqx",
        "outputId": "8e804351-44a0-4be6-be60-6576c329c6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 251248\n",
            "-rw-r--r-- 1 root root 128634549 Jan 26 02:17 best_model.pt\n",
            "drwxr-xr-x 2 root root      4096 Jan 26 02:02 data_cache\n",
            "-rw-r--r-- 1 root root 128634559 Jan 26 02:20 final_model.pt\n",
            "drwxr-xr-x 1 root root      4096 Dec  9 14:42 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0wWlNv17pLtu"
      },
      "outputs": [],
      "source": [
        "def generate_text(model: nn.Module, tokenizer, prompt: str, max_length: int = 100,\n",
        "                 temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize prompt\n",
        "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
        "\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get model predictions\n",
        "            logits = model(generated_ids)\n",
        "            next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            if top_k > 0:\n",
        "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
        "                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n",
        "                next_token_logits[top_k_indices] = top_k_logits\n",
        "\n",
        "            # Apply top-p (nucleus) filtering\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "                sorted_indices_to_remove[0] = 0\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample next token\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to generated sequence - FIX: ensure same dimensions\n",
        "            next_token = next_token.unsqueeze(0)  # Add batch dimension\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "\n",
        "            # Stop if we reach the end token\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "32a9kdikpPUX"
      },
      "outputs": [],
      "source": [
        "def interactive_inference(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"Interactive inference session\"\"\"\n",
        "    print(\"ðŸ¤– Starting interactive inference session\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model, config = load_trained_model(model_path)\n",
        "\n",
        "    # Load tokenizer (assuming we have the same one used during training)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            prompt = input(\"\\n Enter your prompt: \")\n",
        "            if prompt.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"ðŸ‘‹ Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if not prompt.strip():\n",
        "                continue\n",
        "\n",
        "            print(\"ðŸ”„ Generating...\")\n",
        "            generated_text = generate_text(\n",
        "                model, tokenizer, prompt,\n",
        "                max_length=150,\n",
        "                temperature=0.8,\n",
        "                top_k=50,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            print(f\"\\n Generated text:\")\n",
        "            print(f\"ðŸ“ {generated_text}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nðŸ‘‹ Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "2kN2ZQEWpSKG"
      },
      "outputs": [],
      "source": [
        "def demo_inference(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"Run a quick demo of the model's capabilities\"\"\"\n",
        "    print(\"ðŸŽ­ Running inference demo\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model, config = load_trained_model(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Demo prompts\n",
        "    demo_prompts = [\n",
        "        \"The future of artificial intelligence\",\n",
        "        \"Once upon a time in a distant galaxy\",\n",
        "        \"The most important thing to remember is\",\n",
        "        \"In the year 2050, technology will\",\n",
        "        \"The best way to learn programming is\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(demo_prompts, 1):\n",
        "        print(f\"\\n Demo {i}: '{prompt}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        generated_text = generate_text(\n",
        "            model, tokenizer, prompt,\n",
        "            max_length=100,\n",
        "            temperature=0.7,\n",
        "            top_k=40,\n",
        "            top_p=0.85\n",
        "        )\n",
        "\n",
        "        print(f\"ðŸ“ {generated_text}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKOQuocRpU7t",
        "outputId": "eb6bb49a-85c0-4993-c2a8-ad3081d14c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ‰ Found trained model! Running demo...\n",
            "ðŸŽ­ Running inference demo\n",
            " Loading model from final_model.pt\n",
            "âœ… Model loaded successfully\n",
            "   Parameters: 32,150,976\n",
            "   Device: cuda\n",
            "\n",
            " Demo 1: 'The future of artificial intelligence'\n",
            "--------------------------------------------------\n",
            "ðŸ“ The future of artificial intelligence and its its core, and social, and their communities.\n",
            "\n",
            "**What is essential. For instance, which is a single step in the concept of these concepts, the way to the concept of the world, this unit, and their experiences, and the United States.\n",
            "\n",
            "* **Unit 2:\n",
            "* ** **A. By the importance of the same time, these individuals who knows?\n",
            "* **G-based.\n",
            "* **G**: These individuals to the United States\n",
            "\n",
            "\n",
            " Demo 2: 'Once upon a time in a distant galaxy'\n",
            "--------------------------------------------------\n",
            "ðŸ“ Once upon a time in a distant galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy galaxy\n",
            "\n",
            "\n",
            " Demo 3: 'The most important thing to remember is'\n",
            "--------------------------------------------------\n",
            "ðŸ“ The most important thing to remember is an example is the end of the fascinating world of the world of the ` `` function is the `x` is the `(x, we can be used to the `` function.\n",
            "\n",
            "\n",
            "\n",
            "```python\n",
            "\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "\n",
            "\n",
            "\n",
            "```\n",
            "```\n",
            "\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "\n",
            "```python\n",
            "```\n",
            "```python\n",
            "\n",
            "\n",
            " Demo 4: 'In the year 2050, technology will'\n",
            "--------------------------------------------------\n",
            "ðŸ“ In the year 2050, technology will explore the first first first, 10, and the top-in-term.\n",
            "\n",
            "A. This is the same time, 10. However, 1, 120, 100, 1, 2, 10, 200, 1970, 10, 120100000, 400, 20019120000\n",
            "\n",
            "\n",
            " Demo 5: 'The best way to learn programming is'\n",
            "--------------------------------------------------\n",
            "ðŸ“ The best way to learn programming is a group of the same way.\n",
            "\n",
            "```\n",
            "```python\n",
            "\n",
            "Now, the `(x)\n",
            "    return the `y____` function to the ` `x, we will be the `` function.\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "\n",
            "\n",
            "```\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "```\n",
            "```\n",
            "\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "```\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "\n",
            "\n",
            "ðŸ¤– Would you like to try interactive inference? (y/n): y\n",
            "ðŸ¤– Starting interactive inference session\n",
            "Type 'quit' to exit\n",
            " Loading model from final_model.pt\n",
            "âœ… Model loaded successfully\n",
            "   Parameters: 32,150,976\n",
            "   Device: cuda\n",
            "\n",
            " Enter your prompt: hi\n",
            "ðŸ”„ Generating...\n",
            "\n",
            " Generated text:\n",
            "ðŸ“ hi, we're going to find yourself.\n",
            "To begin, we call the most popular. And what they were what makes us.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "As you ever ever heard of these questions about how people might be a person and what you know, we see, it is a group. By doing so much that help us. These are it was a little or other of a long-term and learning about a new. For example, we will explore their own a time you ever heard you can lead to help us that is something called \"Why do.\n",
            "I.\n",
            "\n",
            "The person is a a place, it like you have a long, I feel so that everyone has been used to become more about how you might have been\n",
            "\n",
            " Enter your prompt: what is llm\n",
            "ðŸ”„ Generating...\n",
            "\n",
            " Generated text:\n",
            "ðŸ“ what is llmlmlmlmlmlmlmlmlmlmlmlmlmBoxBoxBoxBoxBoxBoxBoxsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignalsignal Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi Malawi\n",
            "\n",
            " Enter your prompt: \n",
            "\n",
            " Enter your prompt: bye\n",
            "ðŸ”„ Generating...\n",
            "\n",
            " Generated text:\n",
            "ðŸ“ bye, and how they are many people, and other.\n",
            "As the top of a single day, let's dive into how a simple:\n",
            "The \"Well, and why what someone has a family that might have a beautiful. But sometimes, and create a critical aspect of the concept of its role in the ` `test_0, they can be wondering what can get to the `____test__data2 = np. By the ` `` of a function to get into the `b` and `_00, we will have a `____type = 4:\n",
            "\n",
            "```python\n",
            "```\n",
            "\n",
            "```python\n",
            "```python\n",
            "```python\n",
            "```\n",
            "\n",
            "In the `__\n",
            "\n",
            " Enter your prompt: exit\n",
            "ðŸ‘‹ Goodbye!\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if we have a trained model\n",
        "    import os\n",
        "\n",
        "    if os.path.exists(\"final_model.pt\"):\n",
        "        print(\"ðŸŽ‰ Found trained model! Running demo...\")\n",
        "        demo_inference(\"final_model.pt\")\n",
        "\n",
        "        # Optionally run interactive session\n",
        "        response = input(\"\\nðŸ¤– Would you like to try interactive inference? (y/n): \")\n",
        "        if response.lower() in ['y', 'yes']:\n",
        "            interactive_inference(\"final_model.pt\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No trained model found. Please run the training cells first.\")\n",
        "        print(\"ðŸ’¡ Look for 'final_model.pt' or 'best_model.pt' in your directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To7Ms0eoqS_C",
        "outputId": "cfca8993-fdf1-4803-c9c0-01c8aaa548f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bin\t\t\t    kaggle\t\t      opt\t\t sys\n",
            "boot\t\t\t    lib\t\t\t      proc\t\t tmp\n",
            "content\t\t\t    lib32\t\t      python-apt\t tools\n",
            "cuda-keyring_1.1-1_all.deb  lib64\t\t      python-apt.tar.xz  usr\n",
            "datalab\t\t\t    libx32\t\t      root\t\t var\n",
            "dev\t\t\t    media\t\t      run\n",
            "etc\t\t\t    mnt\t\t\t      sbin\n",
            "home\t\t\t    NGC-DL-CONTAINER-LICENSE  srv\n"
          ]
        }
      ],
      "source": [
        "!ls ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIlRmKrdqUHS",
        "outputId": "976c6f37-ec40-4686-d781-4099acb1861a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ],
      "source": [
        "!cat /etc/os-release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjFXLxkeqkSe",
        "outputId": "d878b5cf-a756-41c2-b16a-ab1caed1d479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1ysl2ceqYeH",
        "outputId": "97db9ad3-be26-49d6-ffc6-c1d4cf7df7ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jan 26 02:23:06 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0             34W /   70W |   11754MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEbjN1TPDLGK",
        "outputId": "6fdad8de-c3e1-417c-c898-376f207153e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    print('GPU device not found')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvEZthaODQrN",
        "outputId": "548ab235-4ff1-4665-9d66-602151a4f456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==1.12.0\n",
            "access==1.1.10.post3\n",
            "affine==2.4.0\n",
            "aiofiles==24.1.0\n",
            "aiohappyeyeballs==2.6.1\n",
            "aiohttp==3.13.3\n",
            "aiosignal==1.4.0\n",
            "aiosqlite==0.22.1\n",
            "alabaster==1.0.0\n",
            "albucore==0.0.24\n",
            "albumentations==2.0.8\n",
            "ale-py==0.11.2\n",
            "alembic==1.18.1\n",
            "altair==5.5.0\n",
            "annotated-doc==0.0.4\n",
            "annotated-types==0.7.0\n",
            "antlr4-python3-runtime==4.9.3\n",
            "anyio==4.12.1\n",
            "anywidget==0.9.21\n",
            "apsw==3.51.2.0\n",
            "apswutils==0.1.2\n",
            "argon2-cffi==25.1.0\n",
            "argon2-cffi-bindings==25.1.0\n",
            "array_record==0.8.3\n",
            "arrow==1.4.0\n",
            "arviz==0.22.0\n",
            "astropy==7.2.0\n",
            "astropy-iers-data==0.2026.1.12.0.42.13\n",
            "astunparse==1.6.3\n",
            "atpublic==5.1\n",
            "attrs==25.4.0\n",
            "audioread==3.1.0\n",
            "Authlib==1.6.6\n",
            "autograd==1.8.0\n",
            "babel==2.17.0\n",
            "backcall==0.2.0\n",
            "beartype==0.22.9\n",
            "beautifulsoup4==4.13.5\n",
            "betterproto==2.0.0b6\n",
            "bigframes==2.31.0\n",
            "bigquery-magics==0.10.3\n",
            "bleach==6.3.0\n",
            "blinker==1.9.0\n",
            "blis==1.3.3\n",
            "blobfile==3.1.0\n",
            "blosc2==3.12.2\n",
            "bokeh==3.7.3\n",
            "Bottleneck==1.4.2\n",
            "bqplot==0.12.45\n",
            "branca==0.8.2\n",
            "brotli==1.2.0\n",
            "CacheControl==0.14.4\n",
            "cachetools==6.2.4\n",
            "catalogue==2.0.10\n",
            "certifi==2026.1.4\n",
            "cffi==2.0.0\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.4.4\n",
            "chex==0.1.90\n",
            "clarabel==0.11.1\n",
            "click==8.3.1\n",
            "click-plugins==1.1.1.2\n",
            "cligj==0.7.2\n",
            "cloudpathlib==0.23.0\n",
            "cloudpickle==3.1.2\n",
            "cmake==3.31.10\n",
            "cmdstanpy==1.3.0\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.7\n",
            "contourpy==1.3.3\n",
            "cramjam==2.11.0\n",
            "cryptography==43.0.3\n",
            "cuda-bindings==12.9.5\n",
            "cuda-core==0.3.2\n",
            "cuda-pathfinder==1.3.3\n",
            "cuda-python==12.9.5\n",
            "cuda-toolkit==12.9.1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.10.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "cudf-polars-cu12==25.10.0\n",
            "cufflinks==0.17.3\n",
            "cuml-cu12==25.10.0\n",
            "cupy-cuda12x==13.6.0\n",
            "curl_cffi==0.14.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.6.7\n",
            "cycler==0.12.1\n",
            "cyipopt==1.5.0\n",
            "cymem==2.0.13\n",
            "Cython==3.0.12\n",
            "dask==2025.9.1\n",
            "dask-cuda==25.10.0\n",
            "dask-cudf-cu12==25.10.0\n",
            "dataproc-spark-connect==1.0.1\n",
            "datasets==4.0.0\n",
            "db-dtypes==1.5.0\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.8.15\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "deprecation==2.1.0\n",
            "diffusers==0.36.0\n",
            "dill==0.3.8\n",
            "distributed==2025.9.1\n",
            "distributed-ucxx-cu12==0.46.0\n",
            "distro==1.9.0\n",
            "dlib==19.24.6\n",
            "dm-tree==0.1.9\n",
            "docstring_parser==0.17.0\n",
            "docutils==0.21.2\n",
            "dopamine_rl==4.1.2\n",
            "duckdb==1.3.2\n",
            "earthengine-api==1.5.24\n",
            "easydict==1.13\n",
            "editdistance==0.8.1\n",
            "eerepr==0.1.2\n",
            "einops==0.8.1\n",
            "en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85\n",
            "entrypoints==0.4\n",
            "esda==2.8.1\n",
            "et_xmlfile==2.0.0\n",
            "etils==1.13.0\n",
            "etuples==0.3.10\n",
            "Farama-Notifications==0.0.4\n",
            "fastai==2.8.6\n",
            "fastapi==0.123.10\n",
            "fastcore==1.12.1\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.21.2\n",
            "fastlite==0.2.4\n",
            "fastprogress==1.1.3\n",
            "fastrlock==0.8.3\n",
            "fasttransform==0.0.2\n",
            "ffmpy==1.0.0\n",
            "filelock==3.20.3\n",
            "fiona==1.10.1\n",
            "firebase-admin==6.9.0\n",
            "Flask==3.1.2\n",
            "flatbuffers==25.12.19\n",
            "flax==0.11.2\n",
            "folium==0.20.0\n",
            "fonttools==4.61.1\n",
            "fqdn==1.5.1\n",
            "frozendict==2.4.7\n",
            "frozenlist==1.8.0\n",
            "fsspec==2025.3.0\n",
            "future==1.0.0\n",
            "gast==0.7.0\n",
            "gcsfs==2025.3.0\n",
            "GDAL==3.8.4\n",
            "gdown==5.2.1\n",
            "geemap==0.35.3\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.1\n",
            "geopandas==1.1.2\n",
            "geopy==2.4.1\n",
            "giddy==2.3.8\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.12\n",
            "GitPython==3.1.46\n",
            "glob2==0.7\n",
            "google==3.0.0\n",
            "google-adk==1.21.0\n",
            "google-ai-generativelanguage==0.6.15\n",
            "google-api-core==2.29.0\n",
            "google-api-python-client==2.188.0\n",
            "google-auth==2.43.0\n",
            "google-auth-httplib2==0.3.0\n",
            "google-auth-oauthlib==1.2.2\n",
            "google-cloud-aiplatform==1.130.0\n",
            "google-cloud-appengine-logging==1.8.0\n",
            "google-cloud-audit-log==0.4.0\n",
            "google-cloud-bigquery==3.40.0\n",
            "google-cloud-bigquery-connection==1.20.0\n",
            "google-cloud-bigquery-storage==2.36.0\n",
            "google-cloud-bigtable==2.35.0\n",
            "google-cloud-core==2.5.0\n",
            "google-cloud-dataproc==5.24.0\n",
            "google-cloud-datastore==2.23.0\n",
            "google-cloud-discoveryengine==0.13.12\n",
            "google-cloud-firestore==2.23.0\n",
            "google-cloud-functions==1.22.0\n",
            "google-cloud-language==2.19.0\n",
            "google-cloud-logging==3.13.0\n",
            "google-cloud-monitoring==2.29.0\n",
            "google-cloud-resource-manager==1.16.0\n",
            "google-cloud-secret-manager==2.26.0\n",
            "google-cloud-spanner==3.62.0\n",
            "google-cloud-speech==2.36.0\n",
            "google-cloud-storage==3.8.0\n",
            "google-cloud-trace==1.18.0\n",
            "google-cloud-translate==3.24.0\n",
            "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
            "google-crc32c==1.8.0\n",
            "google-genai==1.55.0\n",
            "google-generativeai==0.8.6\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.8.0\n",
            "googleapis-common-protos==1.72.0\n",
            "googledrivedownloader==1.1.0\n",
            "gradio==5.50.0\n",
            "gradio_client==1.14.0\n",
            "grain==0.2.15\n",
            "graphviz==0.21\n",
            "greenlet==3.3.0\n",
            "groovy==0.1.2\n",
            "grpc-google-iam-v1==0.14.3\n",
            "grpc-interceptor==0.15.4\n",
            "grpcio==1.76.0\n",
            "grpcio-status==1.71.2\n",
            "grpclib==0.4.9\n",
            "gspread==6.2.1\n",
            "gspread-dataframe==4.0.0\n",
            "gym==0.25.2\n",
            "gym-notices==0.1.0\n",
            "gymnasium==1.2.3\n",
            "h11==0.16.0\n",
            "h2==4.3.0\n",
            "h5netcdf==1.8.0\n",
            "h5py==3.15.1\n",
            "hdbscan==0.8.41\n",
            "hf-xet==1.2.0\n",
            "hf_transfer==0.1.9\n",
            "highspy==1.12.0\n",
            "holidays==0.88\n",
            "holoviews==1.22.1\n",
            "hpack==4.1.0\n",
            "html5lib==1.1\n",
            "httpcore==1.0.9\n",
            "httpimport==1.4.1\n",
            "httplib2==0.31.1\n",
            "httptools==0.7.1\n",
            "httpx==0.28.1\n",
            "httpx-sse==0.4.3\n",
            "huggingface-hub==0.36.0\n",
            "humanize==4.15.0\n",
            "hyperframe==6.1.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==9.5.0\n",
            "idna==3.11\n",
            "ImageIO==2.37.2\n",
            "imageio-ffmpeg==0.6.0\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.14.1\n",
            "immutabledict==4.2.2\n",
            "importlib_metadata==8.7.1\n",
            "importlib_resources==6.5.2\n",
            "imutils==0.5.4\n",
            "inequality==1.1.2\n",
            "inflect==7.5.0\n",
            "iniconfig==2.3.0\n",
            "intel-cmplr-lib-ur==2025.3.1\n",
            "intel-openmp==2025.3.1\n",
            "ipyevents==2.0.4\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==6.17.1\n",
            "ipyleaflet==0.20.0\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "isoduration==20.11.0\n",
            "itsdangerous==2.2.0\n",
            "jaraco.classes==3.4.0\n",
            "jaraco.context==6.1.0\n",
            "jaraco.functools==4.4.0\n",
            "jax==0.7.2\n",
            "jax-cuda12-pjrt==0.7.2\n",
            "jax-cuda12-plugin==0.7.2\n",
            "jaxlib==0.7.2\n",
            "jeepney==0.9.0\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.6\n",
            "jiter==0.12.0\n",
            "joblib==1.5.3\n",
            "jsonpatch==1.33\n",
            "jsonpickle==4.1.1\n",
            "jsonpointer==3.0.0\n",
            "jsonschema==4.26.0\n",
            "jsonschema-specifications==2025.9.1\n",
            "jupyter-console==6.6.3\n",
            "jupyter-events==0.12.0\n",
            "jupyter-leaflet==0.20.0\n",
            "jupyter_client==7.4.9\n",
            "jupyter_core==5.9.1\n",
            "jupyter_kernel_gateway @ git+https://github.com/googlecolab/kernel_gateway@b134e9945df25c2dcb98ade9129399be10788671\n",
            "jupyter_server==2.14.0\n",
            "jupyter_server_terminals==0.5.4\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.16\n",
            "jupytext==1.18.1\n",
            "kaggle==1.7.4.5\n",
            "kagglehub==0.3.13\n",
            "keras==3.10.0\n",
            "keras-hub==0.21.1\n",
            "keras-nlp==0.21.1\n",
            "keyring==25.7.0\n",
            "keyrings.google-artifactregistry-auth==1.1.2\n",
            "kiwisolver==1.4.9\n",
            "langchain==1.2.4\n",
            "langchain-core==1.2.7\n",
            "langgraph==1.0.6\n",
            "langgraph-checkpoint==4.0.0\n",
            "langgraph-prebuilt==1.0.6\n",
            "langgraph-sdk==0.3.3\n",
            "langsmith==0.6.4\n",
            "lark==1.3.1\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.10.0-py3-none-manylinux_2_28_x86_64.whl\n",
            "libcugraph-cu12==25.10.1\n",
            "libcuml-cu12==25.10.0\n",
            "libkvikio-cu12==25.10.0\n",
            "libpysal==4.14.1\n",
            "libraft-cu12==25.10.0\n",
            "librmm-cu12==25.10.0\n",
            "librosa==0.11.0\n",
            "libucx-cu12==1.19.0\n",
            "libucxx-cu12==0.46.0\n",
            "lightgbm==4.6.0\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.43.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.7\n",
            "lxml==6.0.2\n",
            "Mako==1.3.10\n",
            "mapclassify==2.10.0\n",
            "Markdown==3.10\n",
            "markdown-it-py==4.0.0\n",
            "MarkupSafe==3.0.3\n",
            "matplotlib==3.10.0\n",
            "matplotlib-inline==0.2.1\n",
            "matplotlib-venn==1.1.2\n",
            "mcp==1.25.0\n",
            "mdit-py-plugins==0.5.0\n",
            "mdurl==0.1.2\n",
            "mgwr==2.2.1\n",
            "miniKanren==1.0.5\n",
            "missingno==0.5.2\n",
            "mistune==3.2.0\n",
            "mizani==0.13.5\n",
            "mkl==2025.3.0\n",
            "ml_dtypes==0.5.4\n",
            "mlxtend==0.23.4\n",
            "mmh3==5.2.0\n",
            "momepy==0.11.0\n",
            "more-itertools==10.8.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.1.2\n",
            "multidict==6.7.0\n",
            "multipledispatch==1.0.0\n",
            "multiprocess==0.70.16\n",
            "multitasking==0.0.12\n",
            "murmurhash==1.0.15\n",
            "music21==9.9.1\n",
            "namex==0.1.0\n",
            "narwhals==2.15.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.3.3\n",
            "nbclient==0.10.4\n",
            "nbconvert==7.16.6\n",
            "nbformat==5.10.4\n",
            "ndindex==1.10.1\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.6.1\n",
            "nibabel==5.3.3\n",
            "nltk==3.9.1\n",
            "notebook==6.5.7\n",
            "notebook_shim==0.2.4\n",
            "numba==0.60.0\n",
            "numba-cuda==0.19.2rc0\n",
            "numexpr==2.14.1\n",
            "numpy==2.0.2\n",
            "nvidia-cublas-cu12==12.6.4.1\n",
            "nvidia-cuda-cccl-cu12==12.9.27\n",
            "nvidia-cuda-cupti-cu12==12.6.80\n",
            "nvidia-cuda-nvcc-cu12==12.5.82\n",
            "nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "nvidia-cuda-runtime-cu12==12.6.77\n",
            "nvidia-cudnn-cu12==9.10.2.21\n",
            "nvidia-cufft-cu12==11.3.0.4\n",
            "nvidia-cufile-cu12==1.11.1.6\n",
            "nvidia-curand-cu12==10.3.7.77\n",
            "nvidia-cusolver-cu12==11.7.1.2\n",
            "nvidia-cusparse-cu12==12.5.4.2\n",
            "nvidia-cusparselt-cu12==0.7.1\n",
            "nvidia-ml-py==13.590.44\n",
            "nvidia-nccl-cu12==2.27.5\n",
            "nvidia-nvjitlink-cu12==12.6.85\n",
            "nvidia-nvshmem-cu12==3.3.20\n",
            "nvidia-nvtx-cu12==12.6.77\n",
            "nvtx==0.2.14\n",
            "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.10.0-py3-none-any.whl\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.3.1\n",
            "omegaconf==2.3.0\n",
            "onemkl-license==2025.3.0\n",
            "openai==2.15.0\n",
            "opencv-contrib-python==4.12.0.88\n",
            "opencv-python==4.12.0.88\n",
            "opencv-python-headless==4.12.0.88\n",
            "openpyxl==3.1.5\n",
            "opentelemetry-api==1.37.0\n",
            "opentelemetry-exporter-gcp-logging==1.11.0a0\n",
            "opentelemetry-exporter-gcp-monitoring==1.11.0a0\n",
            "opentelemetry-exporter-gcp-trace==1.11.0\n",
            "opentelemetry-exporter-otlp-proto-common==1.37.0\n",
            "opentelemetry-exporter-otlp-proto-http==1.37.0\n",
            "opentelemetry-proto==1.37.0\n",
            "opentelemetry-resourcedetector-gcp==1.11.0a0\n",
            "opentelemetry-sdk==1.37.0\n",
            "opentelemetry-semantic-conventions==0.58b0\n",
            "opt_einsum==3.4.0\n",
            "optax==0.2.6\n",
            "optree==0.18.0\n",
            "orbax-checkpoint==0.11.31\n",
            "orjson==3.11.5\n",
            "ormsgpack==1.12.1\n",
            "osqp==1.0.5\n",
            "overrides==7.7.0\n",
            "packaging==25.0\n",
            "pandas==2.2.2\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.30.0\n",
            "pandas-stubs==2.2.2.240909\n",
            "pandocfilters==1.5.1\n",
            "panel==1.8.5\n",
            "param==2.3.1\n",
            "parso==0.8.5\n",
            "parsy==2.2\n",
            "partd==1.4.2\n",
            "patsy==1.0.2\n",
            "peewee==3.19.0\n",
            "peft==0.18.1\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "pillow==11.3.0\n",
            "platformdirs==4.5.1\n",
            "plotly==5.24.1\n",
            "plotnine==0.14.5\n",
            "pluggy==1.6.0\n",
            "plum-dispatch==2.6.1\n",
            "ply==3.11\n",
            "pointpats==2.5.2\n",
            "polars==1.31.0\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "preshed==3.0.12\n",
            "prettytable==3.17.0\n",
            "proglog==0.1.12\n",
            "progressbar2==4.5.0\n",
            "prometheus_client==0.24.1\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.52\n",
            "propcache==0.4.1\n",
            "prophet==1.2.1\n",
            "proto-plus==1.27.0\n",
            "protobuf==5.29.5\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.11\n",
            "psygnal==0.15.1\n",
            "ptyprocess==0.7.0\n",
            "PuLP==3.3.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.9\n",
            "pyarrow==18.1.0\n",
            "pyasn1==0.6.2\n",
            "pyasn1_modules==0.4.2\n",
            "pycairo==1.29.0\n",
            "pycocotools==2.0.11\n",
            "pycparser==2.23\n",
            "pycryptodomex==3.23.0\n",
            "pydantic==2.12.3\n",
            "pydantic-settings==2.12.0\n",
            "pydantic_core==2.41.4\n",
            "pydata-google-auth==1.9.1\n",
            "pydot==4.0.1\n",
            "pydotplus==2.0.2\n",
            "PyDrive2==1.21.3\n",
            "pydub==0.25.1\n",
            "pyerfa==2.0.1.5\n",
            "pygame==2.6.1\n",
            "pygit2==1.19.1\n",
            "Pygments==2.19.2\n",
            "PyGObject==3.48.2\n",
            "PyJWT==2.10.1\n",
            "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.10.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "pylibcugraph-cu12==25.10.1\n",
            "pylibraft-cu12==25.10.0\n",
            "pymc==5.27.0\n",
            "pynndescent==0.6.0\n",
            "pyogrio==0.12.1\n",
            "pyomo==6.9.5\n",
            "PyOpenGL==3.1.10\n",
            "pyOpenSSL==24.2.1\n",
            "pyparsing==3.3.1\n",
            "pyperclip==1.11.0\n",
            "pyproj==3.7.2\n",
            "pysal==25.7\n",
            "pyshp==3.0.3\n",
            "PySocks==1.7.1\n",
            "pyspark==4.0.1\n",
            "pytensor==2.36.3\n",
            "pytest==8.4.2\n",
            "python-apt==0.0.0\n",
            "python-box==7.3.2\n",
            "python-dateutil==2.9.0.post0\n",
            "python-dotenv==1.2.1\n",
            "python-fasthtml==0.12.39\n",
            "python-json-logger==4.0.0\n",
            "python-louvain==0.16\n",
            "python-multipart==0.0.21\n",
            "python-slugify==8.0.4\n",
            "python-snappy==0.7.3\n",
            "python-utils==3.9.1\n",
            "pytz==2025.2\n",
            "pyviz_comms==3.0.6\n",
            "PyWavelets==1.9.0\n",
            "PyYAML==6.0.3\n",
            "pyzmq==26.2.1\n",
            "quantecon==0.10.1\n",
            "raft-dask-cu12==25.10.0\n",
            "rapids-dask-dependency==25.10.0\n",
            "rapids-logger==0.1.19\n",
            "rasterio==1.5.0\n",
            "rasterstats==0.20.0\n",
            "ratelim==0.1.6\n",
            "referencing==0.37.0\n",
            "regex==2025.11.3\n",
            "requests==2.32.4\n",
            "requests-oauthlib==2.0.0\n",
            "requests-toolbelt==1.0.0\n",
            "requirements-parser==0.9.0\n",
            "rfc3339-validator==0.1.4\n",
            "rfc3986-validator==0.1.1\n",
            "rfc3987-syntax==1.1.0\n",
            "rich==13.9.4\n",
            "rmm-cu12==25.10.0\n",
            "roman-numerals==4.1.0\n",
            "roman-numerals-py==4.1.0\n",
            "rpds-py==0.30.0\n",
            "rpy2==3.5.17\n",
            "rsa==4.9.1\n",
            "rtree==1.4.1\n",
            "ruff==0.14.13\n",
            "safehttpx==0.1.7\n",
            "safetensors==0.7.0\n",
            "scikit-image==0.25.2\n",
            "scikit-learn==1.6.1\n",
            "scipy==1.16.3\n",
            "scooby==0.11.0\n",
            "scs==3.2.11\n",
            "seaborn==0.13.2\n",
            "SecretStorage==3.5.0\n",
            "segregation==2.5.3\n",
            "semantic-version==2.10.0\n",
            "Send2Trash==2.1.0\n",
            "sentence-transformers==5.2.0\n",
            "sentencepiece==0.2.1\n",
            "sentry-sdk==2.49.0\n",
            "setuptools==75.2.0\n",
            "shap==0.50.0\n",
            "shapely==2.1.2\n",
            "shellingham==1.5.4\n",
            "simple-parsing==0.1.7\n",
            "simplejson==3.20.2\n",
            "simsimd==6.5.12\n",
            "six==1.17.0\n",
            "sklearn-compat==0.1.5\n",
            "sklearn-pandas==2.2.0\n",
            "slicer==0.0.8\n",
            "smart_open==7.5.0\n",
            "smmap==5.0.2\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==3.0.1\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.13.1\n",
            "soupsieve==2.8.1\n",
            "soxr==1.0.0\n",
            "spacy==3.8.11\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "spaghetti==1.7.6\n",
            "spanner-graph-notebook==1.1.8\n",
            "spglm==1.1.0\n",
            "Sphinx==8.2.3\n",
            "sphinxcontrib-applehelp==2.0.0\n",
            "sphinxcontrib-devhelp==2.0.0\n",
            "sphinxcontrib-htmlhelp==2.1.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==2.0.0\n",
            "sphinxcontrib-serializinghtml==2.0.0\n",
            "spint==1.0.7\n",
            "splot==1.1.7\n",
            "spopt==0.7.0\n",
            "spreg==1.8.5\n",
            "SQLAlchemy==2.0.45\n",
            "sqlalchemy-spanner==1.17.2\n",
            "sqlglot==25.20.2\n",
            "sqlparse==0.5.5\n",
            "srsly==2.5.2\n",
            "sse-starlette==3.1.2\n",
            "stanio==0.5.1\n",
            "starlette==0.50.0\n",
            "statsmodels==0.14.6\n",
            "stringzilla==4.6.0\n",
            "stumpy==1.13.0\n",
            "sympy==1.14.0\n",
            "tables==3.10.2\n",
            "tabulate==0.9.0\n",
            "tbb==2022.3.0\n",
            "tblib==3.2.2\n",
            "tcmlib==1.4.1\n",
            "tenacity==9.1.2\n",
            "tensorboard==2.19.0\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.19.0\n",
            "tensorflow-datasets==4.9.9\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-metadata==1.17.3\n",
            "tensorflow-probability==0.25.0\n",
            "tensorflow-text==2.19.0\n",
            "tensorflow_decision_forests==1.12.0\n",
            "tensorstore==0.1.80\n",
            "termcolor==3.3.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.19.0\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.19.0\n",
            "thinc==8.3.10\n",
            "threadpoolctl==3.6.0\n",
            "tifffile==2026.1.14\n",
            "tiktoken==0.12.0\n",
            "timm==1.0.24\n",
            "tinycss2==1.4.0\n",
            "tobler==0.13.0\n",
            "tokenizers==0.22.2\n",
            "toml==0.10.2\n",
            "tomlkit==0.13.3\n",
            "toolz==0.12.1\n",
            "torch==2.9.0+cu126\n",
            "torchao==0.10.0\n",
            "torchaudio==2.9.0+cu126\n",
            "torchdata==0.11.0\n",
            "torchsummary==1.5.1\n",
            "torchtune==0.6.1\n",
            "torchvision==0.24.0+cu126\n",
            "tornado==6.5.1\n",
            "tqdm==4.67.1\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.3\n",
            "transformers==4.57.6\n",
            "treelite==4.4.1\n",
            "treescope==0.1.10\n",
            "triton==3.5.0\n",
            "tsfresh==0.21.1\n",
            "tweepy==4.16.0\n",
            "typeguard==4.4.4\n",
            "typer==0.21.1\n",
            "typer-slim==0.21.1\n",
            "types-pytz==2025.2.0.20251108\n",
            "types-setuptools==80.9.0.20250822\n",
            "typing-inspection==0.4.2\n",
            "typing_extensions==4.15.0\n",
            "tzdata==2025.3\n",
            "tzlocal==5.3.1\n",
            "uc-micro-py==1.0.3\n",
            "ucxx-cu12==0.46.0\n",
            "umap-learn==0.5.11\n",
            "umf==1.0.2\n",
            "uri-template==1.3.0\n",
            "uritemplate==4.2.0\n",
            "urllib3==2.5.0\n",
            "uuid_utils==0.13.0\n",
            "uvicorn==0.40.0\n",
            "uvloop==0.22.1\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wandb==0.24.0\n",
            "wasabi==1.1.3\n",
            "watchdog==6.0.0\n",
            "watchfiles==1.1.1\n",
            "wcwidth==0.2.14\n",
            "weasel==0.4.3\n",
            "webcolors==25.10.0\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.9.0\n",
            "websockets==15.0.1\n",
            "Werkzeug==3.1.5\n",
            "wheel==0.45.1\n",
            "widgetsnbextension==3.6.10\n",
            "wordcloud==1.9.5\n",
            "wrapt==2.0.1\n",
            "wurlitzer==3.1.1\n",
            "xarray==2025.12.0\n",
            "xarray-einstats==0.9.1\n",
            "xgboost==3.1.3\n",
            "xlrd==2.0.2\n",
            "xxhash==3.6.0\n",
            "xyzservices==2025.11.0\n",
            "yarl==1.22.0\n",
            "ydf==0.14.0\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.66\n",
            "zict==3.0.0\n",
            "zipp==3.23.0\n",
            "zstandard==0.25.0\n"
          ]
        }
      ],
      "source": [
        "pip freeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBsf-cOXH7eN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K79OQQgYHN4p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
